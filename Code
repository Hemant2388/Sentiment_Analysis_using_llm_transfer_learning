{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":93282,"databundleVersionId":11098970,"sourceType":"competition"},{"sourceId":104449,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":68809,"modelId":91102}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nmodel_path = \"/kaggle/input/llama-3.1/transformers/8b-instruct/2\"\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-17T12:46:09.272803Z","iopub.execute_input":"2025-02-17T12:46:09.273056Z","iopub.status.idle":"2025-02-17T12:46:10.548083Z","shell.execute_reply.started":"2025-02-17T12:46:09.273034Z","shell.execute_reply":"2025-02-17T12:46:10.547338Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":" !pip install bitsandbytes evaluate datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T11:38:09.023839Z","iopub.execute_input":"2025-02-17T11:38:09.024314Z","iopub.status.idle":"2025-02-17T11:38:12.645629Z","shell.execute_reply.started":"2025-02-17T11:38:09.024289Z","shell.execute_reply":"2025-02-17T11:38:12.644757Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\nfrom pprint import pprint\nimport math\nimport wandb\n\n#hf\nimport datasets\nfrom datasets import load_dataset, load_from_disk\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM\nfrom transformers import DataCollatorWithPadding\nfrom transformers import LlamaConfig, LlamaForCausalLM,LlamaForSequenceClassification\nfrom transformers import TrainingArguments, Trainer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T12:46:15.136356Z","iopub.execute_input":"2025-02-17T12:46:15.136764Z","iopub.status.idle":"2025-02-17T12:46:41.899099Z","shell.execute_reply.started":"2025-02-17T12:46:15.136738Z","shell.execute_reply":"2025-02-17T12:46:41.898313Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/multi-lingual-sentiment-analysis/train.csv')\ntest_df = pd.read_csv('/kaggle/input/multi-lingual-sentiment-analysis/test.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T12:46:43.735077Z","iopub.execute_input":"2025-02-17T12:46:43.735792Z","iopub.status.idle":"2025-02-17T12:46:43.791625Z","shell.execute_reply.started":"2025-02-17T12:46:43.735763Z","shell.execute_reply":"2025-02-17T12:46:43.790775Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T12:23:37.360214Z","iopub.execute_input":"2025-02-17T12:23:37.360516Z","iopub.status.idle":"2025-02-17T12:23:40.103287Z","shell.execute_reply.started":"2025-02-17T12:23:37.360489Z","shell.execute_reply":"2025-02-17T12:23:40.102622Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"label_map = {\"Negative\": 0, \"Positive\": 1}\ntrain_df[\"label\"] = train_df[\"label\"].map(label_map)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T11:56:31.954502Z","iopub.execute_input":"2025-02-17T11:56:31.954824Z","iopub.status.idle":"2025-02-17T11:56:31.960961Z","shell.execute_reply.started":"2025-02-17T11:56:31.954800Z","shell.execute_reply":"2025-02-17T11:56:31.959785Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import Dataset, DatasetDict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T11:56:32.815790Z","iopub.execute_input":"2025-02-17T11:56:32.816120Z","iopub.status.idle":"2025-02-17T11:56:32.819734Z","shell.execute_reply.started":"2025-02-17T11:56:32.816063Z","shell.execute_reply":"2025-02-17T11:56:32.818964Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = Dataset.from_pandas(train_df)\ntest_dataset = Dataset.from_pandas(test_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T11:56:33.554312Z","iopub.execute_input":"2025-02-17T11:56:33.554637Z","iopub.status.idle":"2025-02-17T11:56:33.573059Z","shell.execute_reply.started":"2025-02-17T11:56:33.554611Z","shell.execute_reply":"2025-02-17T11:56:33.572428Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ds = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T11:56:34.438224Z","iopub.execute_input":"2025-02-17T11:56:34.438530Z","iopub.status.idle":"2025-02-17T11:56:34.442484Z","shell.execute_reply.started":"2025-02-17T11:56:34.438509Z","shell.execute_reply":"2025-02-17T11:56:34.441509Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_path,model_max_length=1024)\n# set pad token id\ntokenizer.pad_token=tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T11:56:35.037621Z","iopub.execute_input":"2025-02-17T11:56:35.037940Z","iopub.status.idle":"2025-02-17T11:56:35.525996Z","shell.execute_reply.started":"2025-02-17T11:56:35.037914Z","shell.execute_reply":"2025-02-17T11:56:35.524965Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tokenize(example):\n    example = tokenizer(example['sentence'],padding=False,truncation=True)\n    return example","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T11:56:35.759974Z","iopub.execute_input":"2025-02-17T11:56:35.760274Z","iopub.status.idle":"2025-02-17T11:56:35.764293Z","shell.execute_reply.started":"2025-02-17T11:56:35.760252Z","shell.execute_reply":"2025-02-17T11:56:35.763401Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T11:56:36.513640Z","iopub.execute_input":"2025-02-17T11:56:36.513938Z","iopub.status.idle":"2025-02-17T11:56:36.519256Z","shell.execute_reply.started":"2025-02-17T11:56:36.513915Z","shell.execute_reply":"2025-02-17T11:56:36.518555Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenized_ds = ds.map(tokenize,batched=True,num_proc=12, remove_columns=['ID', 'sentence', 'language'])\nprint(tokenized_ds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T11:56:37.276560Z","iopub.execute_input":"2025-02-17T11:56:37.276868Z","iopub.status.idle":"2025-02-17T11:56:39.643949Z","shell.execute_reply.started":"2025-02-17T11:56:37.276845Z","shell.execute_reply":"2025-02-17T11:56:39.642922Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ds_split = tokenized_ds['train'].train_test_split(test_size=0.1,seed=42)\nprint(ds_split)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T11:56:39.645480Z","iopub.execute_input":"2025-02-17T11:56:39.645787Z","iopub.status.idle":"2025-02-17T11:56:39.656108Z","shell.execute_reply.started":"2025-02-17T11:56:39.645763Z","shell.execute_reply":"2025-02-17T11:56:39.655122Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":" data_collator = DataCollatorWithPadding(tokenizer,padding=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T11:56:39.657543Z","iopub.execute_input":"2025-02-17T11:56:39.657765Z","iopub.status.idle":"2025-02-17T11:56:39.665266Z","shell.execute_reply.started":"2025-02-17T11:56:39.657746Z","shell.execute_reply":"2025-02-17T11:56:39.664378Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoTokenizer, BitsAndBytesConfig\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",  # Best for LLaMA models\n    bnb_4bit_compute_dtype=torch.float16,  # Can also use bfloat16 if supported\n    bnb_4bit_use_double_quant=True,  # Further reduces memory usage\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T11:56:39.666365Z","iopub.execute_input":"2025-02-17T11:56:39.666662Z","iopub.status.idle":"2025-02-17T11:56:39.680805Z","shell.execute_reply.started":"2025-02-17T11:56:39.666634Z","shell.execute_reply":"2025-02-17T11:56:39.680052Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model = AutoModelForSequenceClassification.from_pretrained(\n#     model_path,\n#     num_labels=2,\n#     pad_token_id=tokenizer.eos_token_id,\n#     quantization_config=bnb_config,\n#     device_map=\"auto\"\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T11:56:42.521824Z","iopub.execute_input":"2025-02-17T11:56:42.522148Z","iopub.status.idle":"2025-02-17T11:56:42.525804Z","shell.execute_reply.started":"2025-02-17T11:56:42.522122Z","shell.execute_reply":"2025-02-17T11:56:42.524751Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from peft import LoraConfig, TaskType, LoraModel\nlora_config = LoraConfig(\n    r=8,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    task_type=TaskType.SEQ_CLS,\n    inference_mode=False,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias='none'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T11:56:43.324302Z","iopub.execute_input":"2025-02-17T11:56:43.324631Z","iopub.status.idle":"2025-02-17T11:56:43.328850Z","shell.execute_reply.started":"2025-02-17T11:56:43.324602Z","shell.execute_reply":"2025-02-17T11:56:43.328162Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from peft import LoraConfig, get_peft_model, TaskType\n# model = get_peft_model(model, lora_config)\n# model.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T11:56:44.315042Z","iopub.execute_input":"2025-02-17T11:56:44.315425Z","iopub.status.idle":"2025-02-17T11:56:44.420837Z","shell.execute_reply.started":"2025-02-17T11:56:44.315398Z","shell.execute_reply":"2025-02-17T11:56:44.419960Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"  # for name,param in model.named_parameters():    \n  #     if name != \"score.weight\":\n  #        param.requires_grad = False\n  #     print(name,param.requires_grad)\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T11:56:45.299455Z","iopub.execute_input":"2025-02-17T11:56:45.299777Z","iopub.status.idle":"2025-02-17T11:56:45.303264Z","shell.execute_reply.started":"2025-02-17T11:56:45.299749Z","shell.execute_reply":"2025-02-17T11:56:45.302355Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  num_parameters = 0\n#  for param in model.parameters():   \n#      if param.requires_grad:\n#          num_parameters += param.numel()\n# print(f'Number of Parameters:{num_parameters}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T11:56:46.200328Z","iopub.execute_input":"2025-02-17T11:56:46.200611Z","iopub.status.idle":"2025-02-17T11:56:46.204163Z","shell.execute_reply.started":"2025-02-17T11:56:46.200589Z","shell.execute_reply":"2025-02-17T11:56:46.203264Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport evaluate\n\n\nmetric = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T11:56:46.935223Z","iopub.execute_input":"2025-02-17T11:56:46.935561Z","iopub.status.idle":"2025-02-17T11:56:47.487356Z","shell.execute_reply.started":"2025-02-17T11:56:46.935538Z","shell.execute_reply":"2025-02-17T11:56:47.486434Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nprint(torch.cuda.is_bf16_supported())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T11:56:48.653208Z","iopub.execute_input":"2025-02-17T11:56:48.653551Z","iopub.status.idle":"2025-02-17T11:56:48.658395Z","shell.execute_reply.started":"2025-02-17T11:56:48.653522Z","shell.execute_reply":"2025-02-17T11:56:48.657523Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#model.gradient_checkpointing_enable()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T11:56:50.060301Z","iopub.execute_input":"2025-02-17T11:56:50.060586Z","iopub.status.idle":"2025-02-17T11:56:50.063946Z","shell.execute_reply.started":"2025-02-17T11:56:50.060565Z","shell.execute_reply":"2025-02-17T11:56:50.063067Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import TrainingArguments\n\n\ntraining_args = TrainingArguments(\n    output_dir='./llama8_multilingual_ft',\n    evaluation_strategy=\"steps\",\n    eval_steps=10,\n    num_train_epochs=3,  # Train for more epochs since data is limited\n    per_device_train_batch_size=2,  # Lowered for memory reasons (P100 GPU)\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=8,  # Effective batch size is 2 * 8 = 16\n    bf16=False,  # P100 doesn't support BF16\n    fp16=False,  # Mixed precision training for memory efficiency\n    gradient_checkpointing=True,  # Save memory on large models\n    optim=\"adamw_torch\",  # More efficient optimizer with torch backend\n    learning_rate=0.1,  # Typically better for LoRA fine-tuning\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_strategy=\"steps\",\n    logging_steps=10,\n    save_steps=10,\n    save_total_limit=2,  # Save less frequently to avoid storage issues\n    report_to='none',\n    dataloader_pin_memory=True,\n    dataloader_num_workers=2,  # Reduce to avoid Kaggle kernel crashes\n    ddp_find_unused_parameters=False,  # Single GPU, this is safe\n    max_grad_norm=1.0,\n    warmup_steps=10,  # Helps stabilize early training\n    lr_scheduler_type=\"cosine\",  # Often works well with fine-tuning LLaMA\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T11:56:51.050765Z","iopub.execute_input":"2025-02-17T11:56:51.051072Z","iopub.status.idle":"2025-02-17T11:56:51.081805Z","shell.execute_reply.started":"2025-02-17T11:56:51.051050Z","shell.execute_reply":"2025-02-17T11:56:51.081053Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# trainer = Trainer(\n#     model=model,\n#     args=training_args,\n#     train_dataset=ds_split[\"train\"],\n#     eval_dataset=ds_split[\"test\"],\n#     data_collator = data_collator,\n#     compute_metrics = compute_metrics\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T11:56:53.865888Z","iopub.execute_input":"2025-02-17T11:56:53.866270Z","iopub.status.idle":"2025-02-17T11:56:53.885718Z","shell.execute_reply.started":"2025-02-17T11:56:53.866242Z","shell.execute_reply":"2025-02-17T11:56:53.885065Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#results = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T11:56:54.741547Z","iopub.execute_input":"2025-02-17T11:56:54.741836Z","iopub.status.idle":"2025-02-17T11:56:54.745529Z","shell.execute_reply.started":"2025-02-17T11:56:54.741814Z","shell.execute_reply":"2025-02-17T11:56:54.744464Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#simple model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T11:57:06.701503Z","iopub.execute_input":"2025-02-17T11:57:06.701798Z","iopub.status.idle":"2025-02-17T11:57:06.705462Z","shell.execute_reply.started":"2025-02-17T11:57:06.701774Z","shell.execute_reply":"2025-02-17T11:57:06.704383Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_path)\nprint(f'Vocab size: {tokenizer.vocab_size}')\nprint(f'Context length: {tokenizer.model_max_length}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T12:46:56.456727Z","iopub.execute_input":"2025-02-17T12:46:56.457003Z","iopub.status.idle":"2025-02-17T12:46:57.118526Z","shell.execute_reply.started":"2025-02-17T12:46:56.456982Z","shell.execute_reply":"2025-02-17T12:46:57.117700Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer.pad_token=tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T12:46:57.699429Z","iopub.execute_input":"2025-02-17T12:46:57.699684Z","iopub.status.idle":"2025-02-17T12:46:57.703238Z","shell.execute_reply.started":"2025-02-17T12:46:57.699662Z","shell.execute_reply":"2025-02-17T12:46:57.702394Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T12:47:02.981759Z","iopub.execute_input":"2025-02-17T12:47:02.982039Z","iopub.status.idle":"2025-02-17T12:47:02.985777Z","shell.execute_reply.started":"2025-02-17T12:47:02.982017Z","shell.execute_reply":"2025-02-17T12:47:02.984836Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    num_labels = 2\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T12:48:59.096523Z","iopub.execute_input":"2025-02-17T12:48:59.096816Z","iopub.status.idle":"2025-02-17T12:50:27.102818Z","shell.execute_reply.started":"2025-02-17T12:48:59.096795Z","shell.execute_reply":"2025-02-17T12:50:27.101893Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import pipeline\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T12:50:29.885663Z","iopub.execute_input":"2025-02-17T12:50:29.886068Z","iopub.status.idle":"2025-02-17T12:50:29.893121Z","shell.execute_reply.started":"2025-02-17T12:50:29.886033Z","shell.execute_reply":"2025-02-17T12:50:29.892261Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions = []\n\nfor idx, row in test_df.iterrows():\n    prompt = prompt = f\"\"\"\nClassify the sentiment of the provided sentence as either **\"Positive\"** or **\"Negative\"** based on its overall tone and context. The sentiment reflects the speaker's satisfaction, opinion, or emotional response.\n\nLanguage: {row['language']}\nSentence: \"{row['sentence']}\"\n\nAnswer with only \"Positive\" or \"Negative\":\n\"\"\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n    outputs = model.generate(**inputs, max_new_tokens=2)\n\n    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n\n    result_cleaned = result.strip().split()[-1].capitalize()# Handles cases like \"Positive\\nExplanation\"\n\n    if result_cleaned not in [\"Positive\", \"Negative\"]:\n        result_cleaned = \"Positive\"  # Fallback or handle it differently\n\n    if result_cleaned == \"Positive\":\n        predictions.append(\"Positive\")\n    elif result_cleaned == \"Negative\":\n        predictions.append(\"Negative\")\n    else:\n        predictions.append(\"Positive\")  # Fallback if it's neither","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T12:52:09.075865Z","iopub.execute_input":"2025-02-17T12:52:09.076210Z","iopub.status.idle":"2025-02-17T12:52:42.092185Z","shell.execute_reply.started":"2025-02-17T12:52:09.076179Z","shell.execute_reply":"2025-02-17T12:52:42.091530Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"(pd.DataFrame(predictions)).value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T12:55:24.030502Z","iopub.execute_input":"2025-02-17T12:55:24.030808Z","iopub.status.idle":"2025-02-17T12:55:24.040794Z","shell.execute_reply.started":"2025-02-17T12:55:24.030784Z","shell.execute_reply":"2025-02-17T12:55:24.039853Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission = pd.DataFrame(columns = ['ID', 'label'])\nsubmission['ID'] = [i for i in range(1, len(predictions)+1)]\nsubmission['label'] = predictions\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T12:55:27.419767Z","iopub.execute_input":"2025-02-17T12:55:27.420036Z","iopub.status.idle":"2025-02-17T12:55:27.438075Z","shell.execute_reply.started":"2025-02-17T12:55:27.420016Z","shell.execute_reply":"2025-02-17T12:55:27.437206Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}